{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/majid9418/majid/blob/main/multi_class_BreakHis2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BQ4vgqlvT-0E"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SnZ4b_sKbivc"
      },
      "outputs": [],
      "source": [
        "!pip install tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jd0dM_u3cHmJ"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "POAtPdJK-1YK",
        "outputId": "585b5aec-c23f-48c6-d219-67d10683054f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2024-09-15 10:16:01--  http://www.inf.ufpr.br/vri/databases/BreaKHis_v1.tar.gz\n",
            "Resolving www.inf.ufpr.br (www.inf.ufpr.br)... 200.17.202.113, 2801:82:80ff:8001:216:ccff:feaa:79\n",
            "Connecting to www.inf.ufpr.br (www.inf.ufpr.br)|200.17.202.113|:80... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://www.inf.ufpr.br/vri/databases/BreaKHis_v1.tar.gz [following]\n",
            "--2024-09-15 10:16:03--  https://www.inf.ufpr.br/vri/databases/BreaKHis_v1.tar.gz\n",
            "Connecting to www.inf.ufpr.br (www.inf.ufpr.br)|200.17.202.113|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4273561758 (4.0G) [application/octet-stream]\n",
            "Saving to: ‘/content/drive/MyDrive/breakhis/BreaKHis_v1.tar.gz’\n",
            "\n",
            "/content/drive/MyDr 100%[===================>]   3.98G  18.3MB/s    in 3m 54s  \n",
            "\n",
            "2024-09-15 10:19:58 (17.4 MB/s) - ‘/content/drive/MyDrive/breakhis/BreaKHis_v1.tar.gz’ saved [4273561758/4273561758]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "# Define the path where you want to save the dataset in your Google Drive\n",
        "dataset_path = '/content/drive/MyDrive/breakhis/BreaKHis_v1.tar.gz'\n",
        "\n",
        "# Make sure the directory exists\n",
        "os.makedirs(os.path.dirname(dataset_path), exist_ok=True)\n",
        "\n",
        "# Download the dataset\n",
        "!wget -O {dataset_path} http://www.inf.ufpr.br/vri/databases/BreaKHis_v1.tar.gz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IAuov3HdAf8U"
      },
      "outputs": [],
      "source": [
        "import tarfile\n",
        "\n",
        "# Extract the dataset\n",
        "with tarfile.open(dataset_path, 'r:gz') as tar:\n",
        "    tar.extractall(path='/content/drive/MyDrive/breakhis/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "q7zsArRsLFU3",
        "outputId": "0e67a40a-e9f4-4e47-8265-eb5be46837b3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 444 images in /content/drive/MyDrive/breakhis/BreaKHis_v1/histology_slides/breast/benign/SOB/adenosis\n",
            "Training set size: 355, Validation set size: 89\n",
            "Found 1014 images in /content/drive/MyDrive/breakhis/BreaKHis_v1/histology_slides/breast/benign/SOB/fibroadenoma\n",
            "Training set size: 811, Validation set size: 203\n",
            "Found 453 images in /content/drive/MyDrive/breakhis/BreaKHis_v1/histology_slides/breast/benign/SOB/phyllodes_tumor\n",
            "Training set size: 362, Validation set size: 91\n",
            "Found 569 images in /content/drive/MyDrive/breakhis/BreaKHis_v1/histology_slides/breast/benign/SOB/tubular_adenoma\n",
            "Training set size: 455, Validation set size: 114\n",
            "Found 3451 images in /content/drive/MyDrive/breakhis/BreaKHis_v1/histology_slides/breast/malignant/SOB/ductal_carcinoma\n",
            "Training set size: 2760, Validation set size: 691\n",
            "Found 626 images in /content/drive/MyDrive/breakhis/BreaKHis_v1/histology_slides/breast/malignant/SOB/lobular_carcinoma\n",
            "Training set size: 500, Validation set size: 126\n",
            "Found 792 images in /content/drive/MyDrive/breakhis/BreaKHis_v1/histology_slides/breast/malignant/SOB/mucinous_carcinoma\n",
            "Training set size: 633, Validation set size: 159\n",
            "Found 560 images in /content/drive/MyDrive/breakhis/BreaKHis_v1/histology_slides/breast/malignant/SOB/papillary_carcinoma\n",
            "Training set size: 448, Validation set size: 112\n",
            "Found 6324 images belonging to 8 classes.\n",
            "Found 1585 images belonging to 8 classes.\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.applications import EfficientNetB5\n",
        "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Dropout\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, LearningRateScheduler\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import shutil\n",
        "import glob\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Define base directory\n",
        "base_dir = '/content/drive/MyDrive/breakhis/BreaKHis_v1/histology_slides/breast'\n",
        "\n",
        "# Define subdirectories for benign and malignant\n",
        "benign_dir = os.path.join(base_dir, 'benign', 'SOB')\n",
        "malignant_dir = os.path.join(base_dir, 'malignant', 'SOB')\n",
        "\n",
        "# Create directories for training and validation data\n",
        "train_dir = os.path.join(base_dir, 'train')\n",
        "val_dir = os.path.join(base_dir, 'validation')\n",
        "\n",
        "os.makedirs(train_dir, exist_ok=True)\n",
        "os.makedirs(val_dir, exist_ok=True)\n",
        "\n",
        "# Create subdirectories for benign and malignant in train and validation folders\n",
        "benign_subclasses = ['adenosis', 'fibroadenoma', 'phyllodes_tumor', 'tubular_adenoma']\n",
        "malignant_subclasses = ['ductal_carcinoma', 'lobular_carcinoma', 'mucinous_carcinoma', 'papillary_carcinoma']\n",
        "\n",
        "for subclass in benign_subclasses + malignant_subclasses:\n",
        "    os.makedirs(os.path.join(train_dir, subclass), exist_ok=True)\n",
        "    os.makedirs(os.path.join(val_dir, subclass), exist_ok=True)\n",
        "\n",
        "# Function to split data and copy to respective directories\n",
        "def split_and_copy_images(source_dir, train_dest_dir, val_dest_dir, split_ratio=0.2):\n",
        "    image_files = glob.glob(os.path.join(source_dir, '**', '*.png'), recursive=True)\n",
        "\n",
        "    # Debugging: Print the number of images found\n",
        "    print(f\"Found {len(image_files)} images in {source_dir}\")\n",
        "\n",
        "    if len(image_files) == 0:\n",
        "        return\n",
        "\n",
        "    train_files, val_files = train_test_split(image_files, test_size=split_ratio, random_state=42)\n",
        "\n",
        "    # Debugging: Print the number of images in training and validation sets\n",
        "    print(f\"Training set size: {len(train_files)}, Validation set size: {len(val_files)}\")\n",
        "\n",
        "    for file in train_files:\n",
        "        shutil.copy(file, train_dest_dir)\n",
        "\n",
        "    for file in val_files:\n",
        "        shutil.copy(file, val_dest_dir)\n",
        "\n",
        "# Split and copy images for each subclass in benign\n",
        "for subclass in benign_subclasses:\n",
        "    benign_sub_dir = os.path.join(benign_dir, subclass)\n",
        "    split_and_copy_images(benign_sub_dir, os.path.join(train_dir, subclass), os.path.join(val_dir, subclass))\n",
        "\n",
        "# Split and copy images for each subclass in malignant\n",
        "for subclass in malignant_subclasses:\n",
        "    malignant_sub_dir = os.path.join(malignant_dir, subclass)\n",
        "    split_and_copy_images(malignant_sub_dir, os.path.join(train_dir, subclass), os.path.join(val_dir, subclass))\n",
        "\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    shear_range=0.3,\n",
        "    zoom_range=0.3,\n",
        "    horizontal_flip=True,\n",
        "    rotation_range=40,\n",
        "    width_shift_range=0.3,\n",
        "    height_shift_range=0.3\n",
        ")\n",
        "\n",
        "validation_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    train_dir,\n",
        "    target_size=(224, 224),\n",
        "    batch_size=32,\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "validation_generator = validation_datagen.flow_from_directory(\n",
        "    val_dir,\n",
        "    target_size=(224, 224),\n",
        "    batch_size=32,\n",
        "    class_mode='categorical'\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7V7_IQr8I7xr"
      },
      "source": [
        "**Additional augmentation** for underrepresented subclasses:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mRgtK-X3ftvi",
        "outputId": "efa526fa-1423-4cce-f2a1-f0b621152cad"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 444 images in /content/drive/MyDrive/breakhis/BreaKHis_v1/histology_slides/breast/benign/SOB/adenosis\n",
            "Training set size: 355, Validation set size: 89\n",
            "Found 1014 images in /content/drive/MyDrive/breakhis/BreaKHis_v1/histology_slides/breast/benign/SOB/fibroadenoma\n",
            "Training set size: 811, Validation set size: 203\n",
            "Found 453 images in /content/drive/MyDrive/breakhis/BreaKHis_v1/histology_slides/breast/benign/SOB/phyllodes_tumor\n",
            "Training set size: 362, Validation set size: 91\n",
            "Found 569 images in /content/drive/MyDrive/breakhis/BreaKHis_v1/histology_slides/breast/benign/SOB/tubular_adenoma\n",
            "Training set size: 455, Validation set size: 114\n",
            "Found 3451 images in /content/drive/MyDrive/breakhis/BreaKHis_v1/histology_slides/breast/malignant/SOB/ductal_carcinoma\n",
            "Training set size: 2760, Validation set size: 691\n",
            "Found 626 images in /content/drive/MyDrive/breakhis/BreaKHis_v1/histology_slides/breast/malignant/SOB/lobular_carcinoma\n",
            "Training set size: 500, Validation set size: 126\n",
            "Found 792 images in /content/drive/MyDrive/breakhis/BreaKHis_v1/histology_slides/breast/malignant/SOB/mucinous_carcinoma\n",
            "Training set size: 633, Validation set size: 159\n",
            "Found 560 images in /content/drive/MyDrive/breakhis/BreaKHis_v1/histology_slides/breast/malignant/SOB/papillary_carcinoma\n",
            "Training set size: 448, Validation set size: 112\n",
            "Found 6324 images belonging to 8 classes.\n",
            "Found 1585 images belonging to 8 classes.\n",
            "{'adenosis': 355, 'fibroadenoma': 811, 'phyllodes_tumor': 362, 'tubular_adenoma': 455, 'ductal_carcinoma': 2760, 'lobular_carcinoma': 500, 'mucinous_carcinoma': 633, 'papillary_carcinoma': 448}\n",
            "Underrepresented classes: ['adenosis', 'phyllodes_tumor', 'tubular_adenoma', 'lobular_carcinoma', 'mucinous_carcinoma', 'papillary_carcinoma']\n",
            "Found 9077 images belonging to 8 classes.\n",
            "Found 1585 images belonging to 8 classes.\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.applications import EfficientNetB5\n",
        "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Dropout\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, LearningRateScheduler, ReduceLROnPlateau\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import shutil\n",
        "import glob\n",
        "from sklearn.model_selection import train_test_split\n",
        "import imgaug.augmenters as iaa\n",
        "\n",
        "# Define base directory\n",
        "base_dir = '/content/drive/MyDrive/breakhis/BreaKHis_v1/histology_slides/breast'\n",
        "\n",
        "# Define subdirectories for benign and malignant\n",
        "benign_dir = os.path.join(base_dir, 'benign', 'SOB')\n",
        "malignant_dir = os.path.join(base_dir, 'malignant', 'SOB')\n",
        "\n",
        "# Create directories for training and validation data\n",
        "train_dir = os.path.join(base_dir, 'train')\n",
        "val_dir = os.path.join(base_dir, 'validation')\n",
        "\n",
        "os.makedirs(train_dir, exist_ok=True)\n",
        "os.makedirs(val_dir, exist_ok=True)\n",
        "\n",
        "# Create subdirectories for benign and malignant in train and validation folders\n",
        "benign_subclasses = ['adenosis', 'fibroadenoma', 'phyllodes_tumor', 'tubular_adenoma']\n",
        "malignant_subclasses = ['ductal_carcinoma', 'lobular_carcinoma', 'mucinous_carcinoma', 'papillary_carcinoma']\n",
        "\n",
        "for subclass in benign_subclasses + malignant_subclasses:\n",
        "    os.makedirs(os.path.join(train_dir, subclass), exist_ok=True)\n",
        "    os.makedirs(os.path.join(val_dir, subclass), exist_ok=True)\n",
        "\n",
        "# Function to split data and copy to respective directories\n",
        "def split_and_copy_images(source_dir, train_dest_dir, val_dest_dir, split_ratio=0.2):\n",
        "    image_files = glob.glob(os.path.join(source_dir, '**', '*.png'), recursive=True)\n",
        "\n",
        "    # Debugging: Print the number of images found\n",
        "    print(f\"Found {len(image_files)} images in {source_dir}\")\n",
        "\n",
        "    if len(image_files) == 0:\n",
        "        return\n",
        "\n",
        "    train_files, val_files = train_test_split(image_files, test_size=split_ratio, random_state=42)\n",
        "\n",
        "    # Debugging: Print the number of images in training and validation sets\n",
        "    print(f\"Training set size: {len(train_files)}, Validation set size: {len(val_files)}\")\n",
        "\n",
        "    for file in train_files:\n",
        "        shutil.copy(file, train_dest_dir)\n",
        "\n",
        "    for file in val_files:\n",
        "        shutil.copy(file, val_dest_dir)\n",
        "\n",
        "# Split and copy images for each subclass in benign\n",
        "for subclass in benign_subclasses:\n",
        "    benign_sub_dir = os.path.join(benign_dir, subclass)\n",
        "    split_and_copy_images(benign_sub_dir, os.path.join(train_dir, subclass), os.path.join(val_dir, subclass))\n",
        "\n",
        "# Split and copy images for each subclass in malignant\n",
        "for subclass in malignant_subclasses:\n",
        "    malignant_sub_dir = os.path.join(malignant_dir, subclass)\n",
        "    split_and_copy_images(malignant_sub_dir, os.path.join(train_dir, subclass), os.path.join(val_dir, subclass))\n",
        "\n",
        "# General data augmentation\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    rotation_range=30,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    brightness_range=[0.9, 1.1],\n",
        ")\n",
        "\n",
        "val_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "# Create general data generators\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    train_dir,\n",
        "    target_size=(224, 224),\n",
        "    batch_size=32,\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "validation_generator = val_datagen.flow_from_directory(\n",
        "    val_dir,\n",
        "    target_size=(224, 224),\n",
        "    batch_size=32,\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "# Calculate the number of images in each subclass\n",
        "subclass_counts = {subclass: len(glob.glob(os.path.join(train_dir, subclass, '*.png'))) for subclass in benign_subclasses + malignant_subclasses}\n",
        "print(subclass_counts)\n",
        "\n",
        "# Determine underrepresented classes\n",
        "mean_count = np.mean(list(subclass_counts.values()))\n",
        "underrepresented_classes = [subclass for subclass, count in subclass_counts.items() if count < mean_count]\n",
        "print(f\"Underrepresented classes: {underrepresented_classes}\")\n",
        "\n",
        "# Custom augmentation pipeline for underrepresented classes\n",
        "aug_pipeline = iaa.Sequential([\n",
        "    iaa.Fliplr(0.5),\n",
        "    iaa.Flipud(0.2),\n",
        "    iaa.Affine(rotate=(-45, 45)),\n",
        "    iaa.Multiply((0.8, 1.2)),\n",
        "    iaa.GaussianBlur(sigma=(0.0, 3.0)),\n",
        "    iaa.AdditiveGaussianNoise(scale=(0.01*255, 0.05*255))\n",
        "])\n",
        "\n",
        "# Augment and add images for underrepresented classes\n",
        "for subclass in underrepresented_classes:\n",
        "    subclass_dir = os.path.join(train_dir, subclass)\n",
        "    images = glob.glob(os.path.join(subclass_dir, '*.png'))\n",
        "\n",
        "    for img_path in images:\n",
        "        img = tf.keras.preprocessing.image.load_img(img_path)\n",
        "        img_array = tf.keras.preprocessing.image.img_to_array(img)\n",
        "        img_array = np.expand_dims(img_array, axis=0)\n",
        "\n",
        "        for _ in range(int(mean_count / len(images))):\n",
        "            aug_images = aug_pipeline(images=img_array.astype(np.uint8))\n",
        "            for aug_img in aug_images:\n",
        "                aug_img_path = os.path.join(subclass_dir, f\"aug_{os.path.basename(img_path)}\")\n",
        "                aug_img = tf.keras.preprocessing.image.array_to_img(aug_img)\n",
        "                aug_img.save(aug_img_path)\n",
        "\n",
        "# Re-create generators after augmentation\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    train_dir,\n",
        "    target_size=(224, 224),\n",
        "    batch_size=32,\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "validation_generator = val_datagen.flow_from_directory(\n",
        "    val_dir,\n",
        "    target_size=(224, 224),\n",
        "    batch_size=32,\n",
        "    class_mode='categorical'\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6ZClNzkAat1N"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.applications import EfficientNetB5\n",
        "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Dropout\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, LearningRateScheduler, ReduceLROnPlateau\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import glob\n",
        "from sklearn.model_selection import train_test_split\n",
        "import seaborn as sns\n",
        "\n",
        "# Define base directory\n",
        "base_dir = '/content/drive/MyDrive/breakhis/BreaKHis_v1/histology_slides/breast'\n",
        "\n",
        "# Define training and validation directories\n",
        "train_dir = os.path.join(base_dir, 'train')\n",
        "val_dir = os.path.join(base_dir, 'validation')\n",
        "\n",
        "# Benign and malignant subclasses\n",
        "benign_subclasses = ['adenosis', 'fibroadenoma', 'phyllodes_tumor', 'tubular_adenoma']\n",
        "malignant_subclasses = ['ductal_carcinoma', 'lobular_carcinoma', 'mucinous_carcinoma', 'papillary_carcinoma']\n",
        "\n",
        "# Ensure previous work is not lost\n",
        "# Re-create generators after augmentation\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    shear_range=0.3,\n",
        "    zoom_range=0.3,\n",
        "    horizontal_flip=True,\n",
        "    rotation_range=40,\n",
        "    width_shift_range=0.3,\n",
        "    height_shift_range=0.3,\n",
        "    brightness_range=[0.9, 1.1],\n",
        ")\n",
        "\n",
        "val_datagen = ImageDataGenerator(rescale=1./255, validation_split=0.5)\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    train_dir,\n",
        "    target_size=(224, 224),\n",
        "    batch_size=32,\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "validation_generator = val_datagen.flow_from_directory(\n",
        "    val_dir,\n",
        "    target_size=(224, 224),\n",
        "    batch_size=32,\n",
        "    class_mode='categorical',\n",
        "    subset='training'\n",
        ")\n",
        "\n",
        "test_generator = val_datagen.flow_from_directory(\n",
        "    val_dir,\n",
        "    target_size=(224, 224),\n",
        "    batch_size=32,\n",
        "    class_mode='categorical',\n",
        "    subset='validation',\n",
        "    shuffle= False\n",
        ")\n",
        "\n",
        "# Define the model\n",
        "model = Sequential([\n",
        "    EfficientNetB5(input_shape=(224, 224, 3), include_top=False, weights='imagenet'),\n",
        "    GlobalAveragePooling2D(),\n",
        "    Dropout(0.8),\n",
        "    Dense(len(benign_subclasses + malignant_subclasses), activation='softmax', kernel_regularizer=tf.keras.regularizers.l2(0.01))\n",
        "])\n",
        "\n",
        "model.compile(optimizer=Adam(learning_rate=1e-4), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Define callbacks\n",
        "checkpoint_filepath = '/content/drive/MyDrive/efficientnet_checkpoints/best_model.keras'\n",
        "checkpoint = ModelCheckpoint(\n",
        "    filepath=checkpoint_filepath,\n",
        "    save_weights_only=False,\n",
        "    monitor='val_loss',\n",
        "    mode='min',\n",
        "    save_best_only=True,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True)\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6)\n",
        "\n",
        "def scheduler(epoch, lr):\n",
        "    if epoch < 10:\n",
        "        return float(lr)\n",
        "    else:\n",
        "        return float(lr * tf.math.exp(-0.1))\n",
        "\n",
        "lr_callback = LearningRateScheduler(scheduler)\n",
        "\n",
        "# Compute class weights\n",
        "class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(train_generator.classes), y=train_generator.classes)\n",
        "class_weights = {i: class_weights[i] for i in range(len(class_weights))}\n",
        "\n",
        "best_test_model_filepath = '/content/drive/MyDrive/efficientnet_checkpoints/best_test_model.keras'\n",
        "\n",
        "# Define the custom callback to evaluate test accuracy during training\n",
        "class TestAccuracyCallback(tf.keras.callbacks.Callback):\n",
        "    def __init__(self, test_generator, save_path):\n",
        "        super(TestAccuracyCallback, self).__init__()\n",
        "        self.test_generator = test_generator\n",
        "        self.best_val_loss = np.inf\n",
        "        self.save_path = save_path\n",
        "        self.best_test_accuracy = 0.0  # Initialize the best test accuracy\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        current_val_loss = logs.get('val_loss')\n",
        "        if current_val_loss < self.best_val_loss:\n",
        "            self.best_val_loss = current_val_loss\n",
        "            test_loss, test_accuracy = self.model.evaluate(self.test_generator, verbose=0)\n",
        "            print(f\"\\nEpoch {epoch+1}: Test loss = {test_loss}, Test accuracy = {test_accuracy * 100:.2f}%\")\n",
        "\n",
        "            if test_accuracy > self.best_test_accuracy:\n",
        "                self.best_test_accuracy = test_accuracy\n",
        "\n",
        "                # Save the model if test accuracy improves\n",
        "                self.model.save(self.save_path)\n",
        "                print(f\"Best model saved for test set at epoch {epoch+1} with Test accuracy: {test_accuracy * 100:.2f}%\")\n",
        "\n",
        "test_accuracy_callback = TestAccuracyCallback(test_generator, save_path=best_test_model_filepath)\n",
        "\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    validation_data=validation_generator,\n",
        "    epochs=32,\n",
        "    class_weight=class_weights,\n",
        "    callbacks=[checkpoint, early_stopping, reduce_lr, lr_callback, test_accuracy_callback]\n",
        ")\n",
        "\n",
        "# Load the best model\n",
        "best_model = tf.keras.models.load_model(checkpoint_filepath)\n",
        "\n",
        "# Evaluate the best model\n",
        "loss, accuracy = best_model.evaluate(validation_generator, verbose=0)\n",
        "print(f'Validation loss: {loss}, Validation accuracy: {accuracy * 100:.2f}%')\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('Training and Validation Accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend(['Training Accuracy', 'Validation Accuracy'])\n",
        "plt.show()\n",
        "\n",
        "# Plot training & validation loss values\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend(['Training Loss', 'Validation Loss'])\n",
        "plt.show()\n",
        "\n",
        "# Evaluate the best model on the test set\n",
        "test_loss, test_accuracy = best_model.evaluate(test_generator, verbose=0)\n",
        "print(f'Test loss: {test_loss}, Test accuracy: {test_accuracy * 100:.2f}%')\n",
        "\n",
        "\n",
        "# Generate predictions and compute confusion matrix\n",
        "test_steps = test_generator.samples // test_generator.batch_size\n",
        "y_pred = best_model.predict(test_generator, steps=test_steps, verbose=1)\n",
        "y_pred_classes = np.argmax(y_pred, axis=1)\n",
        "y_true = test_generator.classes[:len(y_pred_classes)]\n",
        "\n",
        "conf_matrix = confusion_matrix(y_true, y_pred_classes)\n",
        "class_names = list(test_generator.class_indices.keys())\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names, annot_kws={\"size\": 14})\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()\n",
        "\n",
        "# Print classification report\n",
        "report = classification_report(y_true, y_pred_classes, target_names=class_names, digits=3)\n",
        "print(report)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p4OWU5ue8zOL",
        "outputId": "5bd25be6-ad1a-439e-97c8-4f1f29b7a4ea"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VH9mYXLxkXEF"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.applications import EfficientNetB5\n",
        "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Dropout\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, LearningRateScheduler, ReduceLROnPlateau\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import glob\n",
        "from sklearn.model_selection import train_test_split\n",
        "import seaborn as sns\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "\n",
        "# Define base directory\n",
        "base_dir = '/content/drive/MyDrive/breakhis/BreaKHis_v1/histology_slides/breast'\n",
        "\n",
        "# Define training and validation directories\n",
        "train_dir = os.path.join(base_dir, 'train')\n",
        "val_dir = os.path.join(base_dir, 'validation')\n",
        "\n",
        "# Benign and malignant subclasses\n",
        "benign_subclasses = ['adenosis', 'fibroadenoma', 'phyllodes_tumor', 'tubular_adenoma']\n",
        "malignant_subclasses = ['ductal_carcinoma', 'lobular_carcinoma', 'mucinous_carcinoma', 'papillary_carcinoma']\n",
        "\n",
        "# Ensure previous work is not lost\n",
        "# Re-create generators after augmentation\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    shear_range=0.3,\n",
        "    zoom_range=0.3,\n",
        "    horizontal_flip=True,\n",
        "    rotation_range=40,\n",
        "    width_shift_range=0.3,\n",
        "    height_shift_range=0.3,\n",
        "    brightness_range=[0.9, 1.1],\n",
        ")\n",
        "\n",
        "val_datagen = ImageDataGenerator(rescale=1./255, validation_split=0.5)\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    train_dir,\n",
        "    target_size=(224, 224),\n",
        "    batch_size=32,\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "validation_generator = val_datagen.flow_from_directory(\n",
        "    val_dir,\n",
        "    target_size=(224, 224),\n",
        "    batch_size=32,\n",
        "    class_mode='categorical',\n",
        "    subset='training'\n",
        ")\n",
        "\n",
        "test_generator = val_datagen.flow_from_directory(\n",
        "    val_dir,\n",
        "    target_size=(224, 224),\n",
        "    batch_size=32,\n",
        "    class_mode='categorical',\n",
        "    subset='validation',\n",
        "    shuffle= False\n",
        ")\n",
        "\n",
        "'''\n",
        "# Define the base model (EfficientNetB5) and load pre-trained weights\n",
        "base_model = EfficientNetB5(input_shape=(224, 224, 3), include_top=False, weights='imagenet')\n",
        "\n",
        "model = Sequential([\n",
        "    base_model,\n",
        "    GlobalAveragePooling2D(),\n",
        "    Dropout(0.8),\n",
        "    Dense(1, activation='sigmoid', kernel_regularizer=tf.keras.regularizers.l2(0.01))\n",
        "])\n",
        "\n",
        "model.load_weights('/content/drive/MyDrive/efficientnet_checkpoints/best_weights.weights.h5', skip_mismatch=True)\n",
        "for layer in model.layers:\n",
        "    if isinstance(layer, BatchNormalization):\n",
        "        layer.trainable = True  # Fine-tune normalization layers\n",
        "\n",
        "model.pop()\n",
        "model.add(Dense(len(benign_subclasses + malignant_subclasses), activation='softmax', kernel_regularizer=tf.keras.regularizers.l2(0.01)))\n",
        "'''\n",
        "\n",
        "\n",
        "# Define the model\n",
        "model = Sequential([\n",
        "    EfficientNetB5(input_shape=(224, 224, 3), include_top=False, weights='imagenet'),\n",
        "    GlobalAveragePooling2D(),\n",
        "    Dropout(0.8),\n",
        "    Dense(len(benign_subclasses + malignant_subclasses), activation='softmax', kernel_regularizer=tf.keras.regularizers.l2(0.01))\n",
        "])\n",
        "\n",
        "# Build the model\n",
        "model.build((None, 224, 224, 3))\n",
        "\n",
        "model.load_weights('/content/drive/MyDrive/efficientnet_checkpoints/binary_best_model.keras')\n",
        "\n",
        "\n",
        "model.compile(optimizer=Adam(learning_rate=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Define callbacks\n",
        "checkpoint_filepath = '/content/drive/MyDrive/efficientnet_checkpoints/binary_best_model.keras'\n",
        "checkpoint = ModelCheckpoint(\n",
        "    filepath=checkpoint_filepath,\n",
        "    save_weights_only=False,\n",
        "    monitor='val_loss',\n",
        "    mode='min',\n",
        "    save_best_only=True,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True)\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6)\n",
        "\n",
        "def scheduler(epoch, lr):\n",
        "    if epoch < 10:\n",
        "        return float(lr)\n",
        "    else:\n",
        "        return float(lr * tf.math.exp(-0.1))\n",
        "\n",
        "lr_callback = LearningRateScheduler(scheduler)\n",
        "\n",
        "# Compute class weights\n",
        "class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(train_generator.classes), y=train_generator.classes)\n",
        "class_weights = {i: class_weights[i] for i in range(len(class_weights))}\n",
        "\n",
        "\n",
        "best_test_model_filepath = '/content/drive/MyDrive/efficientnet_checkpoints/binary_best_test_model.keras'\n",
        "\n",
        "# Define the custom callback to evaluate test accuracy during training\n",
        "class TestAccuracyCallback(tf.keras.callbacks.Callback):\n",
        "    def __init__(self, test_generator, save_path):\n",
        "        super(TestAccuracyCallback, self).__init__()\n",
        "        self.test_generator = test_generator\n",
        "        self.best_val_loss = np.inf\n",
        "        self.save_path = save_path\n",
        "        self.best_test_accuracy = 0.0  # Initialize the best test accuracy\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        current_val_loss = logs.get('val_loss')\n",
        "        if current_val_loss < self.best_val_loss:\n",
        "            self.best_val_loss = current_val_loss\n",
        "            test_loss, test_accuracy = self.model.evaluate(self.test_generator, verbose=0)\n",
        "            print(f\"\\nEpoch {epoch+1}: Test loss = {test_loss}, Test accuracy = {test_accuracy * 100:.2f}%\")\n",
        "\n",
        "            if test_accuracy > self.best_test_accuracy:\n",
        "                self.best_test_accuracy = test_accuracy\n",
        "\n",
        "                # Save the model if test accuracy improves\n",
        "                self.model.save(self.save_path)\n",
        "                print(f\"Best model saved for test set at epoch {epoch+1} with Test accuracy: {test_accuracy * 100:.2f}%\")\n",
        "\n",
        "test_accuracy_callback = TestAccuracyCallback(test_generator, save_path=best_test_model_filepath)\n",
        "\n",
        "# Train the model\n",
        "remaining_epochs = 32\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    validation_data=validation_generator,\n",
        "    epochs=remaining_epochs,\n",
        "    class_weight=class_weights,\n",
        "    callbacks=[checkpoint, early_stopping, reduce_lr, lr_callback, test_accuracy_callback]\n",
        ")\n",
        "\n",
        "# Load the best model\n",
        "\n",
        "# Evaluate the best model\n",
        "loss, accuracy = model.evaluate(validation_generator, verbose=0)\n",
        "print(f'Validation loss: {loss}, Validation accuracy: {accuracy * 100:.2f}%')\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('Training and Validation Accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend(['Training Accuracy', 'Validation Accuracy'])\n",
        "plt.show()\n",
        "\n",
        "# Plot training & validation loss values\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend(['Training Loss', 'Validation Loss'])\n",
        "plt.show()\n",
        "\n",
        "# Evaluate the best model on the test set\n",
        "test_loss, test_accuracy = model.evaluate(test_generator, verbose=0)\n",
        "print(f'Test loss: {test_loss}, Test accuracy: {test_accuracy * 100:.2f}%')\n",
        "\n",
        "\n",
        "# Generate predictions and compute confusion matrix\n",
        "#test_steps = test_generator.samples // test_generator.batch_size\n",
        "#y_pred = best_model.predict(test_generator, steps=test_steps, verbose=1)\n",
        "y_pred = model.predict(test_generator)\n",
        "y_pred_classes = np.argmax(y_pred, axis=1)\n",
        "y_true = test_generator.classes[:len(y_pred_classes)]\n",
        "\n",
        "conf_matrix = confusion_matrix(y_true, y_pred_classes)\n",
        "class_names = list(test_generator.class_indices.keys())\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names, annot_kws={\"size\": 14})\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()\n",
        "\n",
        "# Print classification report\n",
        "report = classification_report(y_true, y_pred_classes, target_names=class_names)\n",
        "print(report)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "IKI6LehbgjyY"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.applications import EfficientNetB5\n",
        "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Dropout\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, LearningRateScheduler, ReduceLROnPlateau\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import os\n",
        "import glob\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "# Define base directory\n",
        "base_dir = '/content/drive/MyDrive/breakhis/BreaKHis_v1/histology_slides/breast'\n",
        "\n",
        "# Define training and validation directories\n",
        "train_dir = os.path.join(base_dir, 'train')\n",
        "val_dir = os.path.join(base_dir, 'validation')\n",
        "\n",
        "# Benign and malignant subclasses\n",
        "benign_subclasses = ['adenosis', 'fibroadenoma', 'phyllodes_tumor', 'tubular_adenoma']\n",
        "malignant_subclasses = ['ductal_carcinoma', 'lobular_carcinoma', 'mucinous_carcinoma', 'papillary_carcinoma']\n",
        "\n",
        "# Data augmentation for training\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    rotation_range=30,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    brightness_range=[0.9, 1.1],\n",
        ")\n",
        "\n",
        "# For validation and test sets\n",
        "val_datagen = ImageDataGenerator(rescale=1./255, validation_split=0.5)\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    train_dir,\n",
        "    target_size=(224, 224),\n",
        "    batch_size=32,\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "validation_generator = val_datagen.flow_from_directory(\n",
        "    val_dir,\n",
        "    target_size=(224, 224),\n",
        "    batch_size=32,\n",
        "    class_mode='categorical',\n",
        "    subset='training'\n",
        ")\n",
        "\n",
        "test_generator = val_datagen.flow_from_directory(\n",
        "    val_dir,\n",
        "    target_size=(224, 224),\n",
        "    batch_size=32,\n",
        "    class_mode='categorical',\n",
        "    subset='validation',\n",
        "    shuffle=False\n",
        ")\n",
        "\n",
        "# Define the model for transfer learning\n",
        "base_model = EfficientNetB5(input_shape=(224, 224, 3), include_top=False, weights='imagenet')\n",
        "\n",
        "# Freeze the layers of the pre-trained model (except classification head)\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "model = Sequential([\n",
        "    base_model,\n",
        "    GlobalAveragePooling2D(),\n",
        "    Dropout(0.8),\n",
        "    Dense(len(benign_subclasses + malignant_subclasses), activation='softmax', kernel_regularizer=tf.keras.regularizers.l2(0.01))\n",
        "])\n",
        "\n",
        "# Load weights from the binary classification model\n",
        "model.build((None, 224, 224, 3))\n",
        "base_model.load_weights('/content/drive/MyDrive/efficientnet_checkpoints/binary_best_model.keras', skip_mismatch=True)\n",
        "\n",
        "model.compile(optimizer=Adam(learning_rate=1e-4), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Callbacks for training\n",
        "checkpoint_filepath = '/content/drive/MyDrive/efficientnet_checkpoints/binary_best_model.keras'\n",
        "checkpoint = ModelCheckpoint(\n",
        "    filepath=checkpoint_filepath,\n",
        "    save_weights_only=False,\n",
        "    monitor='val_loss',\n",
        "    mode='min',\n",
        "    save_best_only=True,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True)\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6)\n",
        "\n",
        "def scheduler(epoch, lr):\n",
        "    if epoch < 10:\n",
        "        return float(lr)\n",
        "    else:\n",
        "        return float(lr * tf.math.exp(-0.1))\n",
        "\n",
        "lr_callback = LearningRateScheduler(scheduler)\n",
        "\n",
        "# Compute class weights to handle class imbalance\n",
        "class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(train_generator.classes), y=train_generator.classes)\n",
        "class_weights = {i: class_weights[i] for i in range(len(class_weights))}\n",
        "\n",
        "# Define the custom callback to evaluate test accuracy during training\n",
        "class TestAccuracyCallback(tf.keras.callbacks.Callback):\n",
        "    def __init__(self, test_generator, save_path):\n",
        "        super(TestAccuracyCallback, self).__init__()\n",
        "        self.test_generator = test_generator\n",
        "        self.best_val_loss = np.inf\n",
        "        self.save_path = save_path\n",
        "        self.best_test_accuracy = 0.0  # Initialize the best test accuracy\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        current_val_loss = logs.get('val_loss')\n",
        "        if current_val_loss < self.best_val_loss:\n",
        "            self.best_val_loss = current_val_loss\n",
        "            test_loss, test_accuracy = self.model.evaluate(self.test_generator, verbose=0)\n",
        "            print(f\"\\nEpoch {epoch+1}: Test loss = {test_loss}, Test accuracy = {test_accuracy * 100:.2f}%\")\n",
        "\n",
        "            if test_accuracy > self.best_test_accuracy:\n",
        "                self.best_test_accuracy = test_accuracy\n",
        "\n",
        "                # Save the model if test accuracy improves\n",
        "                self.model.save(self.save_path)\n",
        "                print(f\"Best model saved for test set at epoch {epoch+1} with Test accuracy: {test_accuracy * 100:.2f}%\")\n",
        "\n",
        "best_test_model_filepath = '/content/drive/MyDrive/efficientnet_checkpoints/binary_best_test_model.keras'\n",
        "test_accuracy_callback = TestAccuracyCallback(test_generator, save_path=best_test_model_filepath)\n",
        "\n",
        "# Train the model\n",
        "\n",
        "# 3. Train the model with the frozen layers first (fine-tuning head)\n",
        "initial_epochs = 12\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    validation_data=validation_generator,\n",
        "    epochs=initial_epochs,\n",
        "    class_weight=class_weights,\n",
        "    callbacks=[checkpoint, early_stopping, reduce_lr, lr_callback, test_accuracy_callback]\n",
        ")\n",
        "\n",
        "\n",
        "# 4. Gradual unfreezing - unfreeze part of the base model and recompile with lower LR\n",
        "for layer in base_model.layers[-20:]:  # Unfreeze last 20 layers as an example\n",
        "    layer.trainable = True\n",
        "\n",
        "model.compile(optimizer=Adam(learning_rate=1e-5), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Continue training with partial unfreezing\n",
        "fine_tuning_epochs = 20\n",
        "history_fine_tune = model.fit(\n",
        "    train_generator,\n",
        "    validation_data=validation_generator,\n",
        "    epochs=fine_tuning_epochs,\n",
        "    class_weight=class_weights,\n",
        "    callbacks=[checkpoint, early_stopping, reduce_lr, lr_callback, test_accuracy_callback]\n",
        ")\n",
        "\n",
        "# 5. Fully unfreeze and fine-tune all layers\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = True\n",
        "\n",
        "model.compile(optimizer=Adam(learning_rate=1e-6), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Final training with all layers unfrozen\n",
        "final_epochs = 12\n",
        "history_final = model.fit(\n",
        "    train_generator,\n",
        "    validation_data=validation_generator,\n",
        "    epochs=final_epochs,\n",
        "    class_weight=class_weights,\n",
        "    callbacks=[checkpoint, early_stopping, reduce_lr, lr_callback, test_accuracy_callback]\n",
        ")\n",
        "'''\n",
        "# Plot training and validation accuracy\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('Training and Validation Accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend(['Training Accuracy', 'Validation Accuracy'])\n",
        "plt.show()\n",
        "\n",
        "# Plot training & validation loss\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend(['Training Loss', 'Validation Loss'])\n",
        "plt.show()\n",
        "'''\n",
        "# Evaluate the final model on the test set\n",
        "test_loss, test_accuracy = model.evaluate(test_generator, verbose=0)\n",
        "print(f'Test loss: {test_loss}, Test accuracy: {test_accuracy * 100:.2f}%')\n",
        "\n",
        "# Confusion Matrix and Classification Report\n",
        "y_pred = model.predict(test_generator)\n",
        "y_pred_classes = np.argmax(y_pred, axis=1)\n",
        "y_true = test_generator.classes[:len(y_pred_classes)]\n",
        "\n",
        "conf_matrix = confusion_matrix(y_true, y_pred_classes)\n",
        "class_names = list(test_generator.class_indices.keys())\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names, annot_kws={\"size\": 14})\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()\n",
        "\n",
        "# Classification report\n",
        "report = classification_report(y_true, y_pred_classes, target_names=class_names)\n",
        "print(report)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HdKRRnAxj7cp"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications import EfficientNetB5\n",
        "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Dropout\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import os\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Define base directory for test data\n",
        "base_dir = '/content/drive/MyDrive/breakhis/BreaKHis_v1/histology_slides/breast'\n",
        "val_dir = os.path.join(base_dir, 'validation')\n",
        "\n",
        "benign_subclasses = ['adenosis', 'fibroadenoma', 'phyllodes_tumor', 'tubular_adenoma']\n",
        "malignant_subclasses = ['ductal_carcinoma', 'lobular_carcinoma', 'mucinous_carcinoma', 'papillary_carcinoma']\n",
        "\n",
        "# Test data augmentation\n",
        "val_datagen = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1./255, validation_split=0.5)\n",
        "\n",
        "test_generator = val_datagen.flow_from_directory(\n",
        "    val_dir,\n",
        "    target_size=(224, 224),\n",
        "    batch_size=32,\n",
        "    class_mode='categorical',\n",
        "    subset='validation',\n",
        "    shuffle= False\n",
        ")\n",
        "\n",
        "# Define the model\n",
        "model = Sequential([\n",
        "    EfficientNetB5(input_shape=(224, 224, 3), include_top=False, weights='imagenet'),\n",
        "    GlobalAveragePooling2D(),\n",
        "    Dropout(0.8),\n",
        "    Dense(len(benign_subclasses + malignant_subclasses), activation='softmax', kernel_regularizer=tf.keras.regularizers.l2(0.01))\n",
        "])\n",
        "\n",
        "# Build the model\n",
        "model.build((None, 224, 224, 3))\n",
        "\n",
        "model.load_weights('/content/drive/MyDrive/efficientnet_checkpoints/best_test_model.keras')\n",
        "\n",
        "\n",
        "model.compile(optimizer=Adam(learning_rate=1e-4), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "\n",
        "\n",
        "# Evaluate the best model on the test set\n",
        "test_loss, test_accuracy = model.evaluate(test_generator, verbose=0)\n",
        "print(f'Test loss: {test_loss}, Test accuracy: {test_accuracy * 100:.2f}%')\n",
        "\n",
        "\n",
        "# Generate predictions and compute confusion matrix\n",
        "#test_steps = test_generator.samples // test_generator.batch_size\n",
        "#y_pred = best_model.predict(test_generator, steps=test_steps, verbose=1)\n",
        "y_pred = model.predict(test_generator)\n",
        "y_pred_classes = np.argmax(y_pred, axis=1)\n",
        "y_true = test_generator.classes[:len(y_pred_classes)]\n",
        "\n",
        "conf_matrix = confusion_matrix(y_true, y_pred_classes)\n",
        "class_names = list(test_generator.class_indices.keys())\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names, annot_kws={\"size\": 14})\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()\n",
        "\n",
        "# Print classification report\n",
        "report = classification_report(y_true, y_pred_classes, target_names=class_names)\n",
        "print(report)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "41bgP7QXzLvM"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications import EfficientNetB5\n",
        "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Dropout\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import os\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "base_dir = '/content/drive/MyDrive/breakhis/BreaKHis_v1/histology_slides/breast'\n",
        "val_dir = os.path.join(base_dir, 'validation')\n",
        "\n",
        "benign_subclasses = ['adenosis', 'fibroadenoma', 'phyllodes_tumor', 'tubular_adenoma']\n",
        "malignant_subclasses = ['ductal_carcinoma', 'lobular_carcinoma', 'mucinous_carcinoma', 'papillary_carcinoma']\n",
        "\n",
        "val_datagen = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1./255, validation_split=0.5)\n",
        "\n",
        "test_generator = val_datagen.flow_from_directory(\n",
        "    val_dir,\n",
        "    target_size=(224, 224),\n",
        "    batch_size=32,\n",
        "    class_mode='categorical',\n",
        "    subset='validation',\n",
        "    shuffle= False\n",
        ")\n",
        "\n",
        "# Define the number of classes (replace with the actual number)\n",
        "num_classes = len(benign_subclasses + malignant_subclasses)\n",
        "\n",
        "# Rebuild the original model architecture\n",
        "def build_model():\n",
        "    # Load the EfficientNetB5 base model, without the top fully connected layers\n",
        "    base_model = EfficientNetB5(include_top=False, weights='imagenet', input_shape=(224, 224, 3))\n",
        "\n",
        "    # Add new layers on top of the base model\n",
        "    x = base_model.output\n",
        "    x = GlobalAveragePooling2D()(x)  # Apply global average pooling\n",
        "    x = Dropout(0.8)(x)  # Add dropout for regularization\n",
        "    outputs = Dense(num_classes, activation='softmax', kernel_regularizer=tf.keras.regularizers.l2(0.01))(x)\n",
        "\n",
        "    # Create a new model\n",
        "    model = Model(inputs=base_model.input, outputs=outputs)\n",
        "\n",
        "    return model\n",
        "\n",
        "# Create the model architecture\n",
        "model = build_model()\n",
        "\n",
        "# Load the weights from the best model checkpoint\n",
        "checkpoint_filepath = '/content/drive/MyDrive/efficientnet_checkpoints/best_model1.keras'  # Replace with actual path to your best model checkpoint\n",
        "model = tf.keras.models.load_model('/content/drive/MyDrive/efficientnet_checkpoints/best_model1.keras')\n",
        "\n",
        "# Compile the model (you can use the same loss function and metrics as used during training)\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "test_loss, test_accuracy = model.evaluate(test_generator, verbose=0)\n",
        "print(f'Test loss: {test_loss}, Test accuracy: {test_accuracy * 100:.2f}%')\n",
        "\n",
        "# Generate predictions and compute confusion matrix\n",
        "test_steps = test_generator.samples // test_generator.batch_size\n",
        "y_pred = model.predict(test_generator, steps=test_steps, verbose=1)\n",
        "y_pred_classes = np.argmax(y_pred, axis=1)\n",
        "y_true = test_generator.classes[:len(y_pred_classes)]\n",
        "\n",
        "# Confusion matrix and classification report\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "conf_matrix = confusion_matrix(y_true, y_pred_classes)\n",
        "class_names = list(test_generator.class_indices.keys())\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names, annot_kws={\"size\": 14})\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()\n",
        "\n",
        "# Print classification report\n",
        "report = classification_report(y_true, y_pred_classes, target_names=class_names, digits=3)\n",
        "print(report)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}